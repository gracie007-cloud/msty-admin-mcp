#!/usr/bin/env python3
"""
Msty Admin MCP Server v4.1.0

AI-administered Msty Studio Desktop management system with database insights,
configuration management, hardware optimization, and Claude Desktop sync.

Phase 1: Foundational Tools (Read-Only)
- detect_msty_installation
- read_msty_database
- list_configured_tools
- get_model_providers
- analyse_msty_health
- get_server_status

Phase 2: Configuration Management
- export_tool_config
- sync_claude_preferences
- generate_persona
- import_tool_config

Phase 3: Automation Bridge
- query_local_ai_service
- list_available_models
- get_sidecar_status
- chat_with_local_model
- recommend_model

Phase 4: Intelligence Layer
- get_model_performance_metrics
- analyse_conversation_patterns
- compare_model_responses
- optimise_knowledge_stacks
- suggest_persona_improvements

Phase 5: Tiered AI Workflow
- run_calibration_test
- evaluate_response_quality
- identify_handoff_triggers
- get_calibration_history

Created by Pineapple ðŸ AI Administration System
"""

import json
import logging
import os
import platform
import sqlite3
import urllib.request
import urllib.error
import time
from dataclasses import dataclass, asdict, field
from datetime import datetime
from pathlib import Path
from typing import Any, Optional, Dict, List

import psutil
from mcp.server.fastmcp import FastMCP

# Import Phase 4 & 5 utilities
from .phase4_5_tools import (
    init_metrics_db,
    record_model_metric,
    get_model_metrics_summary,
    save_calibration_result,
    get_calibration_results,
    record_handoff_trigger,
    get_handoff_triggers,
    evaluate_response_heuristic,
    CALIBRATION_PROMPTS,
    QUALITY_RUBRIC
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("msty-admin-mcp")

# Initialize FastMCP server
mcp = FastMCP("msty-admin")

# =============================================================================
# Constants
# =============================================================================

SERVER_VERSION = "5.1.0"

# Configurable via environment variables
SIDECAR_HOST = os.environ.get("MSTY_SIDECAR_HOST", "127.0.0.1")
SIDECAR_PROXY_PORT = int(os.environ.get("MSTY_PROXY_PORT", 11932))
LOCAL_AI_SERVICE_PORT = int(os.environ.get("MSTY_AI_PORT", 11964))
SIDECAR_TIMEOUT = int(os.environ.get("MSTY_TIMEOUT", 10))

# Msty 2.4.0+ ports (services built into main app)
MLX_SERVICE_PORT = int(os.environ.get("MSTY_MLX_PORT", 11973))
LLAMACPP_SERVICE_PORT = int(os.environ.get("MSTY_LLAMACPP_PORT", 11454))
VIBE_PROXY_PORT = int(os.environ.get("MSTY_VIBE_PORT", 8317))

# =============================================================================
# Data Classes
# =============================================================================

@dataclass
class MstyInstallation:
    """Msty Studio Desktop installation details"""
    installed: bool
    version: Optional[str] = None
    app_path: Optional[str] = None
    data_path: Optional[str] = None
    sidecar_path: Optional[str] = None
    database_path: Optional[str] = None
    mlx_models_path: Optional[str] = None
    is_running: bool = False
    sidecar_running: bool = False
    platform_info: dict = field(default_factory=dict)


@dataclass
class MstyHealthReport:
    """Msty Studio health analysis"""
    overall_status: str
    database_status: dict = field(default_factory=dict)
    storage_status: dict = field(default_factory=dict)
    model_cache_status: dict = field(default_factory=dict)
    recommendations: list = field(default_factory=list)
    timestamp: str = ""


@dataclass
class DatabaseStats:
    """Statistics from Msty database"""
    total_conversations: int = 0
    total_messages: int = 0
    total_personas: int = 0
    total_prompts: int = 0
    total_knowledge_stacks: int = 0
    total_tools: int = 0
    database_size_mb: float = 0.0
    last_activity: Optional[str] = None


@dataclass
class PersonaConfig:
    """Msty persona configuration structure"""
    name: str
    description: str = ""
    system_prompt: str = ""
    temperature: float = 0.7
    top_p: float = 0.9
    max_tokens: int = 4096
    model_preference: Optional[str] = None
    knowledge_stacks: list = field(default_factory=list)
    tools_enabled: list = field(default_factory=list)
    created_at: str = ""
    updated_at: str = ""


# =============================================================================
# Path Resolution Utilities
# =============================================================================

def get_msty_paths() -> dict:
    """Get all relevant Msty Studio paths for macOS"""
    home = Path.home()

    paths = {
        "app": Path("/Applications/MstyStudio.app"),
        "app_alt": Path("/Applications/Msty Studio.app"),
        "data": home / "Library/Application Support/MstyStudio",
        "sidecar": home / "Library/Application Support/MstySidecar",
    }

    resolved = {}
    for key, path in paths.items():
        resolved[key] = str(path) if path.exists() else None

    # Enhanced database detection for Msty 2.4.0+
    resolved["database"] = None
    resolved["database_type"] = None

    # Check environment variable override first
    env_db_path = os.environ.get("MSTY_DATABASE_PATH")
    if env_db_path and Path(env_db_path).exists():
        resolved["database"] = env_db_path
        resolved["database_type"] = "env_override"

    # Check old Sidecar path
    if not resolved["database"] and resolved["sidecar"]:
        sidecar_db = Path(resolved["sidecar"]) / "SharedStorage"
        if sidecar_db.exists():
            resolved["database"] = str(sidecar_db)
            resolved["database_type"] = "sidecar_shared"

    # Search for database files in data directory (Msty 2.4.0+)
    if not resolved["database"] and resolved["data"]:
        data_path = Path(resolved["data"])

        # Common database file patterns for Msty 2.4.0+
        db_patterns = [
            "msty.db",
            "msty.sqlite",
            "msty.sqlite3",
            "data.db",
            "app.db",
            "storage.db",
            "MstyStudio.db",
            "*.db",
            "**/*.db",
            "databases/*.db",
            "db/*.db",
            "storage/*.db",
        ]

        for pattern in db_patterns:
            if "*" in pattern:
                # Glob pattern
                matches = list(data_path.glob(pattern))
                if matches:
                    # Prefer larger files (more likely to be the main database)
                    matches.sort(key=lambda p: p.stat().st_size, reverse=True)
                    resolved["database"] = str(matches[0])
                    resolved["database_type"] = f"glob:{pattern}"
                    resolved["all_databases"] = [str(m) for m in matches[:5]]
                    break
            else:
                # Exact path
                db_file = data_path / pattern
                if db_file.exists():
                    resolved["database"] = str(db_file)
                    resolved["database_type"] = "direct"
                    break

    # Also check for SQLite files in Containers (sandboxed apps)
    if not resolved["database"]:
        containers_path = home / "Library/Containers/ai.msty.MstyStudio/Data/Library/Application Support"
        if containers_path.exists():
            for db_file in containers_path.glob("**/*.db"):
                resolved["database"] = str(db_file)
                resolved["database_type"] = "container"
                break

    if resolved["data"]:
        mlx_path = Path(resolved["data"]) / "models-mlx"
        resolved["mlx_models"] = str(mlx_path) if mlx_path.exists() else None
    else:
        resolved["mlx_models"] = None

    return resolved


# =============================================================================
# Response Caching
# =============================================================================

class ResponseCache:
    """Simple TTL cache for API responses"""

    def __init__(self, default_ttl: int = 30):
        self._cache: Dict[str, tuple] = {}  # key -> (value, expiry_time)
        self._default_ttl = default_ttl

    def get(self, key: str) -> Optional[Any]:
        """Get cached value if not expired"""
        if key in self._cache:
            value, expiry = self._cache[key]
            if time.time() < expiry:
                return value
            else:
                del self._cache[key]
        return None

    def set(self, key: str, value: Any, ttl: int = None) -> None:
        """Cache a value with TTL"""
        ttl = ttl or self._default_ttl
        self._cache[key] = (value, time.time() + ttl)

    def invalidate(self, key: str = None) -> None:
        """Clear specific key or entire cache"""
        if key:
            self._cache.pop(key, None)
        else:
            self._cache.clear()

    def stats(self) -> dict:
        """Get cache statistics"""
        now = time.time()
        valid = sum(1 for _, (_, exp) in self._cache.items() if exp > now)
        return {"total_entries": len(self._cache), "valid_entries": valid}


# Global cache instance
_response_cache = ResponseCache(default_ttl=30)


def get_cached_models() -> Optional[dict]:
    """Get cached model list"""
    return _response_cache.get("models_list")


def cache_models(models_data: dict, ttl: int = 60) -> None:
    """Cache model list for TTL seconds"""
    _response_cache.set("models_list", models_data, ttl)


# =============================================================================
# Model Tagging System
# =============================================================================

# Model tags based on known model characteristics
MODEL_TAGS = {
    # By name patterns
    "patterns": {
        "fast": ["granite", "phi", "gemma-2b", "qwen.*0.6b", "qwen.*1.5b", "tiny", "small", "mini"],
        "quality": ["70b", "72b", "405b", "235b", "253b", "120b", "opus", "sonnet"],
        "coding": ["coder", "codex", "deepseek-coder", "starcoder", "code"],
        "creative": ["creative", "writer", "story", "hermes", "nous"],
        "reasoning": ["r1", "thinking", "reason", "o1", "deepseek-r1"],
        "embedding": ["embed", "bge", "nomic", "e5", "gte"],
        "vision": ["vision", "llava", "image", "visual"],
        "long_context": ["longcat", "yarn", "longrope"],
    },
    # Manual overrides for specific models
    "overrides": {
        "mlx-community/granite-3.3-2b-instruct-4bit": ["fast", "general"],
        "mlx-community/Qwen3-32B-MLX-4bit": ["quality", "general", "reasoning"],
        "GGorman/DeepSeek-Coder-V2-Instruct-Q4-mlx": ["coding", "quality"],
    }
}


def get_model_tags(model_id: str) -> List[str]:
    """
    Get tags for a model based on its ID.
    Tags help with smart model selection.
    """
    tags = set()
    model_lower = model_id.lower()

    # Check manual overrides first
    if model_id in MODEL_TAGS["overrides"]:
        return MODEL_TAGS["overrides"][model_id]

    # Check patterns
    import re
    for tag, patterns in MODEL_TAGS["patterns"].items():
        for pattern in patterns:
            if re.search(pattern, model_lower):
                tags.add(tag)
                break

    # Add size-based tags
    if any(size in model_lower for size in ["2b", "3b", "4b", "7b", "8b"]):
        tags.add("small")
    elif any(size in model_lower for size in ["13b", "14b", "27b", "32b", "34b"]):
        tags.add("medium")
    elif any(size in model_lower for size in ["70b", "72b", "120b", "235b", "405b"]):
        tags.add("large")

    # Default to general if no specific tags
    if not tags:
        tags.add("general")

    return list(tags)


def find_models_by_tag(tag: str, models: List[dict] = None) -> List[dict]:
    """
    Find models matching a specific tag.
    If models not provided, fetches from cache or API.
    """
    if models is None:
        cached = get_cached_models()
        if cached:
            models = cached.get("models", [])
        else:
            # Fetch fresh
            services = get_available_service_ports()
            models = []
            for service_name, service_info in services.items():
                if service_info["available"]:
                    response = make_api_request("/v1/models", port=service_info["port"])
                    if response.get("success"):
                        data = response.get("data", {})
                        if isinstance(data, dict) and "data" in data:
                            for m in data["data"]:
                                m["_service"] = service_name
                                m["_port"] = service_info["port"]
                            models.extend(data["data"])

    matching = []
    for model in models:
        model_id = model.get("id", "")
        model_tags = get_model_tags(model_id)
        if tag in model_tags:
            model["_tags"] = model_tags
            matching.append(model)

    return matching


def is_process_running(process_name: str) -> bool:
    """Check if a process is running by name"""
    for proc in psutil.process_iter(['name']):
        try:
            if process_name.lower() in proc.info['name'].lower():
                return True
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue
    return False


def is_local_ai_available(port: int = None, timeout: int = 2) -> bool:
    """
    Check if Local AI Service is available by attempting to connect.
    Works with Msty 2.4.0+ where services are built into main app.
    """
    port = port or LOCAL_AI_SERVICE_PORT
    try:
        url = f"http://{SIDECAR_HOST}:{port}/v1/models"
        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=timeout) as response:
            return response.status == 200
    except:
        return False


def get_available_service_ports() -> dict:
    """
    Check which Msty AI services are available (Msty 2.4.0+).
    Returns dict with service status and ports.
    """
    services = {
        "local_ai": {"port": LOCAL_AI_SERVICE_PORT, "available": False},
        "mlx": {"port": MLX_SERVICE_PORT, "available": False},
        "llamacpp": {"port": LLAMACPP_SERVICE_PORT, "available": False},
        "vibe_proxy": {"port": VIBE_PROXY_PORT, "available": False},
    }

    for name, info in services.items():
        services[name]["available"] = is_local_ai_available(info["port"])

    return services


# =============================================================================
# Database Operations
# =============================================================================

def get_database_connection(db_path: str) -> Optional[sqlite3.Connection]:
    """Get a read-only connection to Msty database"""
    if not db_path or not Path(db_path).exists():
        return None
    try:
        conn = sqlite3.connect(f"file:{db_path}?mode=ro", uri=True)
        conn.row_factory = sqlite3.Row
        return conn
    except sqlite3.Error as e:
        logger.error(f"Database connection error: {e}")
        return None


def query_database(db_path: str, query: str, params: tuple = ()) -> list:
    """Execute a read-only query on the Msty database"""
    conn = get_database_connection(db_path)
    if not conn:
        return []
    try:
        cursor = conn.cursor()
        cursor.execute(query, params)
        return [dict(row) for row in cursor.fetchall()]
    except sqlite3.Error as e:
        logger.error(f"Query error: {e}")
        return []
    finally:
        conn.close()


def get_table_names(db_path: str) -> list:
    """Get all table names from the database"""
    query = "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name"
    results = query_database(db_path, query)
    return [r['name'] for r in results]


def get_table_row_count(db_path: str, table_name: str) -> int:
    """Get row count for a specific table"""
    if not table_name.isidentifier():
        return 0
    query = f"SELECT COUNT(*) as count FROM {table_name}"
    results = query_database(db_path, query)
    return results[0]['count'] if results else 0


# =============================================================================
# API Request Helper
# =============================================================================

def make_api_request(
    endpoint: str,
    port: int = LOCAL_AI_SERVICE_PORT,
    method: str = "GET",
    data: Optional[Dict] = None,
    timeout: int = SIDECAR_TIMEOUT,
    host: str = None
) -> Dict[str, Any]:
    """Make HTTP request to Sidecar or Local AI Service API"""
    host = host or SIDECAR_HOST
    url = f"http://{host}:{port}{endpoint}"
    
    try:
        if method == "GET":
            req = urllib.request.Request(url)
        else:
            json_data = json.dumps(data).encode('utf-8') if data else None
            req = urllib.request.Request(url, data=json_data, method=method)
            req.add_header('Content-Type', 'application/json')
        
        with urllib.request.urlopen(req, timeout=timeout) as response:
            response_data = response.read().decode('utf-8')
            return {
                "success": True,
                "status_code": response.status,
                "data": json.loads(response_data) if response_data else None
            }
    except urllib.error.URLError as e:
        logger.warning(f"Connection failed to {url}: {e.reason}")
        return {"success": False, "error": f"Connection failed: {e.reason}"}
    except urllib.error.HTTPError as e:
        # Capture response body for better debugging
        try:
            error_body = e.read().decode('utf-8', errors='ignore')[:200]
            logger.warning(f"HTTP {e.code} on {endpoint}: {error_body}")
        except:
            error_body = None
        return {"success": False, "error": f"HTTP {e.code}: {e.reason}", "status_code": e.code, "error_body": error_body}
    except json.JSONDecodeError:
        return {"success": True, "status_code": 200, "data": response_data}
    except Exception as e:
        logger.error(f"Unexpected error calling {url}: {e}")
        return {"success": False, "error": str(e)}


# =============================================================================
# Config Helpers
# =============================================================================

def read_claude_desktop_config() -> dict:
    """Read Claude Desktop's MCP configuration"""
    config_path = Path.home() / "Library/Application Support/Claude/claude_desktop_config.json"
    if not config_path.exists():
        return {"error": "Claude Desktop config not found"}
    try:
        with open(config_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        return {"error": str(e)}


def sanitize_path(path: str) -> str:
    """Replace home directory with $HOME for portability"""
    home = str(Path.home())
    if path and path.startswith(home):
        return path.replace(home, "$HOME", 1)
    return path


def expand_path(path: str) -> str:
    """Expand $HOME and ~ in paths"""
    if path:
        path = path.replace("$HOME", str(Path.home()))
        path = os.path.expanduser(path)
    return path


# =============================================================================
# Phase 1: Foundational Tools (Read-Only)
# =============================================================================

@mcp.tool()
def detect_msty_installation() -> str:
    """
    Detect and analyse Msty Studio Desktop installation.
    
    Returns comprehensive information about:
    - Installation status and paths
    - Application version (if detectable)
    - Running status of Msty Studio and Sidecar
    - Platform information
    - Data directory locations
    
    This is the first tool to run when working with Msty Admin MCP.
    """
    paths = get_msty_paths()
    app_path = paths.get("app") or paths.get("app_alt")
    installed = app_path is not None
    
    version = None
    if app_path:
        plist_path = Path(app_path) / "Contents/Info.plist"
        if plist_path.exists():
            try:
                import plistlib
                with open(plist_path, 'rb') as f:
                    plist = plistlib.load(f)
                    version = plist.get('CFBundleShortVersionString', plist.get('CFBundleVersion'))
            except:
                pass
    
    platform_info = {
        "system": platform.system(),
        "release": platform.release(),
        "machine": platform.machine(),
        "is_apple_silicon": platform.machine() in ["arm64", "aarch64"],
    }
    
    installation = MstyInstallation(
        installed=installed,
        version=version,
        app_path=app_path,
        data_path=paths.get("data"),
        sidecar_path=paths.get("sidecar"),
        database_path=paths.get("database"),
        mlx_models_path=paths.get("mlx_models"),
        is_running=is_process_running("MstyStudio"),
        sidecar_running=is_local_ai_available(),  # Msty 2.4.0+ has services built-in
        platform_info=platform_info
    )
    
    return json.dumps(asdict(installation), indent=2)


@mcp.tool()
def read_msty_database(
    query_type: str = "stats",
    table_name: Optional[str] = None,
    limit: int = 50
) -> str:
    """
    Query the Msty Studio database for insights.
    
    Args:
        query_type: Type of query to run:
            - "stats": Get overall database statistics
            - "tables": List all tables in the database
            - "conversations": Get recent conversations
            - "personas": List configured personas
            - "prompts": List saved prompts
            - "tools": List configured MCP tools
            - "custom": Query a specific table (requires table_name)
        table_name: Table name for custom queries (only used with query_type="custom")
        limit: Maximum number of results to return (default: 50)
    
    Returns:
        JSON string with query results
    """
    paths = get_msty_paths()
    db_path = paths.get("database")
    
    if not db_path:
        return json.dumps({"error": "Msty database not found"})
    
    result = {"query_type": query_type, "database_path": db_path}
    
    try:
        if query_type == "tables":
            tables = get_table_names(db_path)
            result["tables"] = [{"name": t, "row_count": get_table_row_count(db_path, t)} for t in tables]
            
        elif query_type == "stats":
            tables = get_table_names(db_path)
            stats = DatabaseStats()
            table_mapping = {
                "chat_sessions": "total_conversations",
                "conversations": "total_conversations",
                "messages": "total_messages",
                "chat_messages": "total_messages",
                "personas": "total_personas",
                "prompts": "total_prompts",
                "knowledge_stacks": "total_knowledge_stacks",
                "tools": "total_tools",
            }
            for table in tables:
                for pattern, attr in table_mapping.items():
                    if pattern in table.lower():
                        count = get_table_row_count(db_path, table)
                        setattr(stats, attr, getattr(stats, attr, 0) + count)
                        break
            db_file = Path(db_path)
            stats.database_size_mb = round(db_file.stat().st_size / (1024 * 1024), 2)
            result["stats"] = asdict(stats)
            result["available_tables"] = tables
            
        elif query_type == "custom" and table_name:
            if table_name in get_table_names(db_path):
                result["data"] = query_database(db_path, f"SELECT * FROM {table_name} LIMIT ?", (limit,))
            else:
                result["error"] = f"Table '{table_name}' not found"
        else:
            table_map = {
                "conversations": ["chat_sessions", "conversations"],
                "personas": ["personas"],
                "prompts": ["prompts", "prompt_library"],
                "tools": ["tools", "mcp_tools"],
            }
            if query_type in table_map:
                for t in table_map[query_type]:
                    if t in get_table_names(db_path):
                        result[query_type] = query_database(db_path, f"SELECT * FROM {t} LIMIT ?", (limit,))
                        break
    except Exception as e:
        result["error"] = str(e)
    
    return json.dumps(result, indent=2, default=str)


@mcp.tool()
def list_configured_tools() -> str:
    """
    List all MCP tools configured in Msty Studio's Toolbox.
    
    Returns detailed information about each tool including:
    - Tool ID and name
    - Configuration (command, args, env vars)
    - Status and notes
    """
    paths = get_msty_paths()
    db_path = paths.get("database")
    
    if not db_path:
        return json.dumps({"error": "Msty database not found", "tools": []})
    
    result = {"database_path": db_path, "tools": [], "tool_count": 0}
    tables = get_table_names(db_path)
    tool_tables = [t for t in tables if "tool" in t.lower() or "mcp" in t.lower()]
    
    for table in tool_tables:
        tools = query_database(db_path, f"SELECT * FROM {table}")
        if tools:
            result["tools"].extend(tools)
            break
    
    result["tool_count"] = len(result["tools"])
    return json.dumps(result, indent=2, default=str)


@mcp.tool()
def get_model_providers() -> str:
    """
    Get configured AI model providers in Msty Studio.
    
    Returns information about:
    - Local models (MLX, Ollama connections)
    - Remote providers (OpenAI, Anthropic, etc.)
    - Model configurations and parameters
    
    Note: API keys are NOT returned for security reasons.
    """
    paths = get_msty_paths()
    mlx_path = paths.get("mlx_models")
    
    result = {
        "local_models": {"mlx_available": mlx_path is not None, "mlx_models": []},
        "remote_providers": []
    }
    
    if mlx_path and Path(mlx_path).exists():
        for model_dir in Path(mlx_path).iterdir():
            if model_dir.is_dir():
                result["local_models"]["mlx_models"].append({
                    "name": model_dir.name,
                    "size_mb": sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file()) / (1024 * 1024)
                })
    
    return json.dumps(result, indent=2, default=str)


@mcp.tool()
def analyse_msty_health() -> str:
    """
    Perform comprehensive health analysis of Msty Studio installation.
    
    Checks:
    - Database integrity and size
    - Storage usage and available space
    - Model cache status
    - Application and Sidecar status
    - Configuration completeness
    
    Returns a health report with status and recommendations.
    """
    paths = get_msty_paths()
    health = MstyHealthReport(overall_status="unknown", timestamp=datetime.now().isoformat())
    issues, warnings = [], []
    
    if not paths.get("app") and not paths.get("app_alt"):
        health.overall_status = "critical"
        health.recommendations.append("Install Msty Studio Desktop from https://msty.ai")
        return json.dumps(asdict(health), indent=2)
    
    db_path = paths.get("database")
    if db_path and Path(db_path).exists():
        db_size_mb = Path(db_path).stat().st_size / (1024 * 1024)
        health.database_status = {"exists": True, "size_mb": round(db_size_mb, 2)}
        if db_size_mb > 500:
            warnings.append(f"Database is large ({db_size_mb:.0f}MB)")
    
    # Check all Msty 2.4.0+ services
    services = get_available_service_ports()
    health.recommendations.append(f"Msty Studio: {'Running âœ…' if is_process_running('MstyStudio') or is_process_running('Msty') else 'Not running'}")
    health.recommendations.append(f"Local AI Service (port {LOCAL_AI_SERVICE_PORT}): {'Running âœ…' if services['local_ai']['available'] else 'Not running'}")
    health.recommendations.append(f"MLX Service (port {MLX_SERVICE_PORT}): {'Running âœ…' if services['mlx']['available'] else 'Not running'}")
    health.recommendations.append(f"LLaMA.cpp Service (port {LLAMACPP_SERVICE_PORT}): {'Running âœ…' if services['llamacpp']['available'] else 'Not running'}")
    
    health.overall_status = "critical" if issues else ("warning" if warnings else "healthy")
    return json.dumps(asdict(health), indent=2)


@mcp.tool()
def get_server_status() -> str:
    """
    Get the current status of the Msty Admin MCP server.
    
    Returns server information including:
    - Server name and version
    - Available tools
    - Msty installation status summary
    - Current capabilities
    """
    paths = get_msty_paths()
    
    return json.dumps({
        "server": {
            "name": "msty-admin-mcp",
            "version": SERVER_VERSION,
            "phase": "Phase 5+ - Enhanced",
            "author": "Pineapple ðŸ + Claude"
        },
        "available_tools": {
            "phase_1_foundational": ["detect_msty_installation", "read_msty_database", "list_configured_tools", "get_model_providers", "analyse_msty_health", "get_server_status", "scan_database_locations"],
            "phase_2_configuration": ["export_tool_config", "sync_claude_preferences", "generate_persona", "import_tool_config"],
            "phase_3_automation": ["get_sidecar_status", "list_available_models", "query_local_ai_service", "chat_with_local_model", "recommend_model", "list_model_tags", "find_model_by_tag"],
            "phase_3_cache": ["get_cache_stats", "clear_cache"],
            "phase_4_intelligence": ["get_model_performance_metrics", "analyse_conversation_patterns", "compare_model_responses", "optimise_knowledge_stacks", "suggest_persona_improvements"],
            "phase_5_calibration": ["run_calibration_test", "evaluate_response_quality", "identify_handoff_triggers", "get_calibration_history"]
        },
        "tool_count": 28,
        "msty_status": {
            "installed": paths.get("app") is not None or paths.get("app_alt") is not None,
            "database_available": paths.get("database") is not None,
            "database_type": paths.get("database_type"),
            "local_ai_available": is_local_ai_available()
        },
        "cache_stats": _response_cache.stats()
    }, indent=2)


# =============================================================================
# Phase 2: Configuration Management Tools
# =============================================================================

@mcp.tool()
def export_tool_config(
    tool_name: Optional[str] = None,
    source: str = "claude",
    output_format: str = "msty",
    include_env: bool = False
) -> str:
    """
    Export MCP tool configurations for backup or sync.
    
    Args:
        tool_name: Specific tool to export (None = all tools)
        source: Where to read config from ("claude" or "msty")
        output_format: Output format ("msty", "claude", or "raw")
        include_env: Include environment variables (may contain secrets)
    
    Returns:
        JSON with tool configurations ready for import
    """
    result = {"source": source, "output_format": output_format, "timestamp": datetime.now().isoformat(), "tools": []}
    
    if source == "claude":
        config = read_claude_desktop_config()
        if "error" in config:
            return json.dumps(config, indent=2)
        
        for name, server_config in config.get("mcpServers", {}).items():
            if tool_name and name != tool_name:
                continue
            tool = {
                "name": name,
                "command": server_config.get("command", ""),
                "args": server_config.get("args", []),
            }
            if include_env:
                tool["env"] = server_config.get("env", {})
            result["tools"].append(tool)
    
    result["tool_count"] = len(result["tools"])
    return json.dumps(result, indent=2, default=str)


@mcp.tool()
def sync_claude_preferences(
    output_path: Optional[str] = None,
    include_memory_protocol: bool = True,
    include_tool_priorities: bool = True
) -> str:
    """
    Convert Claude Desktop preferences to Msty persona format.
    
    Args:
        output_path: Optional path to save the persona JSON file
        include_memory_protocol: Include memory system integration instructions
        include_tool_priorities: Include MCP tool priority order
    
    Returns:
        JSON with Msty persona configuration
    """
    sections = ["# AI Assistant Persona - Opus Style\n\nBritish English, conversational tone, quality over quantity."]
    
    if include_memory_protocol:
        sections.append("\n## Memory Protocol\nCheck memory MCP at conversation start. Store important info proactively.")
    
    if include_tool_priorities:
        sections.append("\n## Tool Priorities\n1. Memory MCP\n2. Filesystem MCP\n3. Other specialised MCPs")
    
    persona = PersonaConfig(
        name="Opus Style Assistant",
        description="Claude Opus behaviour patterns",
        system_prompt="\n".join(sections),
        created_at=datetime.now().isoformat(),
        updated_at=datetime.now().isoformat()
    )
    
    result = {"persona": asdict(persona), "system_prompt_length": len(persona.system_prompt)}
    
    if output_path:
        try:
            with open(expand_path(output_path), 'w') as f:
                json.dump(result["persona"], f, indent=2)
            result["saved_to"] = output_path
        except Exception as e:
            result["save_error"] = str(e)
    
    return json.dumps(result, indent=2)


@mcp.tool()
def generate_persona(
    name: str,
    description: str = "",
    base_template: str = "opus",
    custom_instructions: Optional[str] = None,
    temperature: float = 0.7,
    model_preference: Optional[str] = None,
    output_path: Optional[str] = None
) -> str:
    """
    Generate a complete Msty persona configuration.
    
    Args:
        name: Name for the persona
        description: Brief description of the persona's purpose
        base_template: Starting template ("opus", "minimal", "coder", "writer")
        custom_instructions: Additional instructions to append
        temperature: Model temperature (0.0-1.0)
        model_preference: Preferred model identifier
        output_path: Optional path to save the persona JSON
    
    Returns:
        JSON with complete persona configuration ready for Msty import
    """
    templates = {
        "opus": "AI assistant with British English, quality focus, executive mindset.",
        "minimal": "Helpful AI assistant.",
        "coder": "Development assistant with code review focus.",
        "writer": "Writing assistant with British English standards."
    }
    
    if base_template not in templates:
        return json.dumps({"error": f"Unknown template: {base_template}", "available": list(templates.keys())})
    
    system_prompt = templates[base_template]
    if custom_instructions:
        system_prompt += f"\n\n{custom_instructions}"
    
    persona = PersonaConfig(
        name=name,
        description=description or f"{base_template} persona",
        system_prompt=system_prompt,
        temperature=temperature,
        model_preference=model_preference,
        created_at=datetime.now().isoformat(),
        updated_at=datetime.now().isoformat()
    )
    
    result = {"persona": asdict(persona), "base_template": base_template}
    
    if output_path:
        try:
            Path(expand_path(output_path)).parent.mkdir(parents=True, exist_ok=True)
            with open(expand_path(output_path), 'w') as f:
                json.dump(result["persona"], f, indent=2)
            result["saved_to"] = output_path
        except Exception as e:
            result["save_error"] = str(e)
    
    return json.dumps(result, indent=2)


@mcp.tool()
def import_tool_config(
    config_json: Optional[str] = None,
    config_file: Optional[str] = None,
    source: str = "claude",
    dry_run: bool = True
) -> str:
    """
    Import MCP tool configurations into Msty Studio.
    
    Args:
        config_json: JSON string with tool configuration(s)
        config_file: Path to JSON file with tool configuration(s)
        source: Source format ("claude", "msty", "auto")
        dry_run: If True, validate only without importing (default: True)
    
    Returns:
        JSON with validation results and import instructions
    """
    result = {"dry_run": dry_run, "validation": {"valid": False, "errors": []}, "tools_to_import": []}
    
    config = None
    if config_json:
        try:
            config = json.loads(config_json)
        except json.JSONDecodeError as e:
            result["validation"]["errors"].append(f"Invalid JSON: {e}")
            return json.dumps(result, indent=2)
    elif config_file:
        try:
            with open(expand_path(config_file), 'r') as f:
                config = json.load(f)
        except Exception as e:
            result["validation"]["errors"].append(str(e))
            return json.dumps(result, indent=2)
    else:
        config = read_claude_desktop_config()
        if "error" in config:
            result["validation"]["errors"].append(config["error"])
            return json.dumps(result, indent=2)
    
    tools = []
    if "mcpServers" in config:
        for name, sc in config["mcpServers"].items():
            tools.append({"name": name, "config": {"command": sc.get("command", ""), "args": sc.get("args", [])}})
    
    result["tools_to_import"] = tools
    result["validation"]["valid"] = len(tools) > 0
    result["validation"]["tool_count"] = len(tools)
    
    return json.dumps(result, indent=2, default=str)


# =============================================================================
# Phase 3: Automation Bridge - Sidecar API Integration
# =============================================================================

@mcp.tool()
def get_sidecar_status() -> str:
    """
    Get comprehensive status of Msty Local AI Services.

    Returns:
        - Msty Studio process status
        - Local AI Service availability (port 11964)
        - MLX Service availability (port 11973)
        - LLaMA.cpp Service availability (port 11454)
        - Vibe CLI Proxy availability (port 8317)
        - Available models
        - Port information

    Note: Msty 2.4.0+ has services built into the main app, no separate Sidecar needed.
    """
    # Check all services (Msty 2.4.0+ architecture)
    services = get_available_service_ports()

    result = {
        "timestamp": datetime.now().isoformat(),
        "msty_studio": {"process_running": is_process_running("MstyStudio") or is_process_running("Msty")},
        "services": services,
        "local_ai_service": {"port": LOCAL_AI_SERVICE_PORT, "reachable": False, "models_available": 0},
        "recommendations": []
    }

    # Check if any service is available
    any_service_available = any(s["available"] for s in services.values())

    if not any_service_available:
        result["recommendations"].append("Start Msty Studio and enable Local AI services in Settings")
        return json.dumps(result, indent=2)

    # Try to get models from Local AI Service first, then fallback to other services
    for service_name, service_info in services.items():
        if service_info["available"]:
            models_response = make_api_request("/v1/models", port=service_info["port"], timeout=5)
            if models_response.get("success"):
                result["local_ai_service"]["reachable"] = True
                result["local_ai_service"]["port"] = service_info["port"]
                result["local_ai_service"]["service"] = service_name
                data = models_response.get("data", {})
                if isinstance(data, dict) and "data" in data:
                    result["local_ai_service"]["models_available"] = len(data["data"])
                    result["local_ai_service"]["model_list"] = [m.get("id") for m in data["data"]]
                break

    return json.dumps(result, indent=2)


@mcp.tool()
def list_available_models() -> str:
    """
    List all AI models available through Msty's Local AI Services.

    Checks all available services (Local AI, MLX, LLaMA.cpp, Vibe Proxy)
    and returns models from the first available service.

    Returns detailed information about each model.
    """
    result = {"timestamp": datetime.now().isoformat(), "models": [], "model_count": 0}

    # Check all services
    services = get_available_service_ports()
    any_service_available = any(s["available"] for s in services.values())

    if not any_service_available:
        result["error"] = "No Local AI services are running. Start Msty Studio and enable services in Settings."
        return json.dumps(result, indent=2)

    # Collect models from ALL available services
    all_models = []
    services_with_models = {}

    for service_name, service_info in services.items():
        if service_info["available"]:
            response = make_api_request("/v1/models", port=service_info["port"])
            if response.get("success"):
                data = response.get("data", {})
                if isinstance(data, dict) and "data" in data:
                    service_models = data["data"]
                    # Tag each model with its service
                    for model in service_models:
                        model["_service"] = service_name
                        model["_port"] = service_info["port"]
                    all_models.extend(service_models)
                    services_with_models[service_name] = {
                        "port": service_info["port"],
                        "model_count": len(service_models),
                        "models": [m.get("id") for m in service_models]
                    }

    result["models"] = all_models
    result["model_count"] = len(all_models)
    result["by_service"] = services_with_models

    if not all_models:
        result["error"] = "No models found on any service"

    return json.dumps(result, indent=2)


@mcp.tool()
def query_local_ai_service(
    endpoint: str = "/v1/models",
    method: str = "GET",
    request_body: Optional[str] = None
) -> str:
    """
    Query the Sidecar Local AI Service API directly.
    
    Args:
        endpoint: API endpoint (e.g., "/v1/models", "/v1/chat/completions")
        method: HTTP method (GET, POST)
        request_body: JSON string for POST requests
    
    Returns:
        Raw API response with status information
    """
    if not is_local_ai_available():
        return json.dumps({"error": "No Local AI service is running. Start Msty Studio and enable services."})

    data = json.loads(request_body) if request_body else None
    response = make_api_request(endpoint, port=LOCAL_AI_SERVICE_PORT, method=method, data=data, timeout=30)
    
    return json.dumps({"endpoint": endpoint, "method": method, "response": response}, indent=2, default=str)


def get_chat_port_for_model(model_id: str) -> int:
    """
    Determine which port to use for a given model.
    Checks all services to find where the model is available.
    """
    services = get_available_service_ports()

    for service_name, service_info in services.items():
        if service_info["available"]:
            response = make_api_request("/v1/models", port=service_info["port"], timeout=5)
            if response.get("success"):
                data = response.get("data", {})
                if isinstance(data, dict) and "data" in data:
                    model_ids = [m.get("id") for m in data["data"]]
                    if model_id in model_ids:
                        return service_info["port"]

    # Default to MLX port if available, then LLaMA.cpp, then local_ai
    if services["mlx"]["available"]:
        return MLX_SERVICE_PORT
    if services["llamacpp"]["available"]:
        return LLAMACPP_SERVICE_PORT
    return LOCAL_AI_SERVICE_PORT


def get_first_chat_model() -> tuple:
    """
    Find the first available chat model (not embedding model).
    Returns (model_id, port) or (None, None) if none found.
    """
    services = get_available_service_ports()

    # Prefer MLX and LLaMA.cpp services for chat models
    for service_name in ["mlx", "llamacpp", "vibe_proxy", "local_ai"]:
        service_info = services.get(service_name, {})
        if service_info.get("available"):
            response = make_api_request("/v1/models", port=service_info["port"], timeout=5)
            if response.get("success"):
                data = response.get("data", {})
                if isinstance(data, dict) and "data" in data:
                    for model in data["data"]:
                        model_id = model.get("id", "")
                        # Skip embedding models
                        if "embed" in model_id.lower() or "bge" in model_id.lower() or "nomic" in model_id.lower():
                            continue
                        return (model_id, service_info["port"])

    return (None, None)


@mcp.tool()
def chat_with_local_model(
    message: str,
    model: Optional[str] = None,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 1024,
    track_metrics: bool = True
) -> str:
    """
    Send a chat message to a local model via Sidecar.

    Args:
        message: The user message to send
        model: Model ID to use (if None, uses first available chat model)
        system_prompt: Optional system prompt for context
        temperature: Sampling temperature (0.0-1.0)
        max_tokens: Maximum tokens in response
        track_metrics: Record performance metrics (default: True)

    Returns:
        Model response with timing and token information
    """
    result = {"timestamp": datetime.now().isoformat(), "request": {"message": message[:100] + "..." if len(message) > 100 else message}}

    # Check if any service is available
    services = get_available_service_ports()
    any_available = any(s["available"] for s in services.values())

    if not any_available:
        result["error"] = "No Local AI service is running. Start Msty Studio and enable services."
        return json.dumps(result, indent=2)

    # Find model and appropriate port
    if model:
        port = get_chat_port_for_model(model)
    else:
        model, port = get_first_chat_model()
        if not model:
            result["error"] = "No chat models available (only embedding models found)"
            return json.dumps(result, indent=2)

    result["request"]["model"] = model
    result["request"]["port"] = port

    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": message})

    request_data = {"model": model, "messages": messages, "temperature": temperature, "max_tokens": max_tokens, "stream": False}

    start_time = time.time()
    response = make_api_request("/v1/chat/completions", port=port, method="POST", data=request_data, timeout=120)
    elapsed_time = time.time() - start_time
    
    result["timing"] = {"elapsed_seconds": round(elapsed_time, 2)}
    
    if response.get("success"):
        data = response.get("data", {})
        if "choices" in data and data["choices"]:
            msg = data["choices"][0].get("message", {})
            content = msg.get("content", "") or msg.get("reasoning", "")
            result["response"] = {"content": content, "finish_reason": data["choices"][0].get("finish_reason")}
        
        if "usage" in data:
            result["usage"] = data["usage"]
            completion_tokens = data["usage"].get("completion_tokens", 0)
            result["timing"]["tokens_per_second"] = round(completion_tokens / max(elapsed_time, 0.1), 1)
            
            if track_metrics:
                try:
                    init_metrics_db()
                    record_model_metric(
                        model_id=model,
                        prompt_tokens=data["usage"].get("prompt_tokens", 0),
                        completion_tokens=completion_tokens,
                        latency_seconds=elapsed_time,
                        success=True,
                        use_case="chat"
                    )
                except:
                    pass
    else:
        result["error"] = response.get("error")
        if track_metrics:
            try:
                init_metrics_db()
                record_model_metric(model_id=model, latency_seconds=elapsed_time, success=False, error_message=response.get("error"))
            except:
                pass
    
    return json.dumps(result, indent=2, default=str)


@mcp.tool()
def recommend_model(use_case: str = "general", max_size_gb: Optional[float] = None) -> str:
    """
    Get model recommendations based on use case and hardware.
    
    Args:
        use_case: Type of work ("general", "coding", "writing", "analysis", "fast")
        max_size_gb: Maximum model size in GB (optional)
    
    Returns:
        Recommended models with installation instructions
    """
    model_db = {
        "general": [{"id": "qwen2.5:32b", "size_gb": 19, "quality": "very good"}, {"id": "qwen2.5:7b", "size_gb": 4.5, "quality": "good"}],
        "coding": [{"id": "qwen2.5-coder:32b", "size_gb": 19, "quality": "excellent"}, {"id": "qwen2.5-coder:7b", "size_gb": 4.5, "quality": "good"}],
        "writing": [{"id": "qwen2.5:32b", "size_gb": 19, "quality": "very good"}],
        "analysis": [{"id": "qwen2.5:32b", "size_gb": 19, "quality": "very good"}],
        "fast": [{"id": "qwen3:0.6b", "size_gb": 0.5, "quality": "basic"}, {"id": "gemma3:4b", "size_gb": 3, "quality": "acceptable"}]
    }
    
    if use_case not in model_db:
        return json.dumps({"error": f"Unknown use case", "valid": list(model_db.keys())})
    
    recommendations = model_db[use_case]
    if max_size_gb:
        recommendations = [m for m in recommendations if m["size_gb"] <= max_size_gb]
    
    return json.dumps({"use_case": use_case, "recommendations": recommendations}, indent=2)


@mcp.tool()
def list_model_tags(model_id: Optional[str] = None) -> str:
    """
    Get tags for models to help with smart selection.

    Tags include: fast, quality, coding, creative, reasoning, embedding, vision, long_context, small, medium, large, general

    Args:
        model_id: Specific model to get tags for (None = all models with their tags)

    Returns:
        Model tags and available tag categories
    """
    result = {
        "timestamp": datetime.now().isoformat(),
        "available_tags": list(MODEL_TAGS["patterns"].keys()) + ["small", "medium", "large", "general"],
    }

    if model_id:
        tags = get_model_tags(model_id)
        result["model_id"] = model_id
        result["tags"] = tags
    else:
        # Get all models and their tags
        services = get_available_service_ports()
        models_with_tags = []

        for service_name, service_info in services.items():
            if service_info["available"]:
                response = make_api_request("/v1/models", port=service_info["port"])
                if response.get("success"):
                    data = response.get("data", {})
                    if isinstance(data, dict) and "data" in data:
                        for m in data["data"]:
                            mid = m.get("id", "")
                            tags = get_model_tags(mid)
                            models_with_tags.append({
                                "model_id": mid,
                                "tags": tags,
                                "service": service_name,
                                "port": service_info["port"]
                            })

        result["models"] = models_with_tags
        result["model_count"] = len(models_with_tags)

        # Summarize by tag
        tag_counts = {}
        for m in models_with_tags:
            for tag in m["tags"]:
                tag_counts[tag] = tag_counts.get(tag, 0) + 1
        result["tag_summary"] = tag_counts

    return json.dumps(result, indent=2)


@mcp.tool()
def find_model_by_tag(
    tag: str,
    prefer_fast: bool = False,
    exclude_embedding: bool = True
) -> str:
    """
    Find models matching a specific tag.

    Args:
        tag: Tag to search for (fast, quality, coding, creative, reasoning, etc.)
        prefer_fast: If True, sort smaller/faster models first
        exclude_embedding: If True, exclude embedding models from results

    Returns:
        List of matching models with their details
    """
    result = {"timestamp": datetime.now().isoformat(), "tag": tag, "models": []}

    # Get all models
    services = get_available_service_ports()
    all_models = []

    for service_name, service_info in services.items():
        if service_info["available"]:
            response = make_api_request("/v1/models", port=service_info["port"])
            if response.get("success"):
                data = response.get("data", {})
                if isinstance(data, dict) and "data" in data:
                    for m in data["data"]:
                        m["_service"] = service_name
                        m["_port"] = service_info["port"]
                        all_models.append(m)

    # Filter by tag
    matching = find_models_by_tag(tag, all_models)

    # Optionally exclude embedding models
    if exclude_embedding:
        matching = [m for m in matching if "embedding" not in m.get("_tags", [])]

    # Sort if prefer_fast
    if prefer_fast:
        # Sort by size indicators in name (smaller first)
        def size_key(m):
            mid = m.get("id", "").lower()
            if any(s in mid for s in ["0.6b", "1b", "2b", "3b"]):
                return 0
            if any(s in mid for s in ["4b", "7b", "8b"]):
                return 1
            if any(s in mid for s in ["13b", "14b", "27b", "32b"]):
                return 2
            return 3
        matching.sort(key=size_key)

    result["models"] = matching
    result["match_count"] = len(matching)

    if not matching:
        result["note"] = f"No models found with tag '{tag}'. Available tags: fast, quality, coding, creative, reasoning, embedding, vision, small, medium, large, general"

    return json.dumps(result, indent=2)


@mcp.tool()
def get_cache_stats() -> str:
    """
    Get statistics about the response cache.

    Returns cache hit/miss information and current entries.
    Useful for debugging and performance monitoring.
    """
    stats = _response_cache.stats()
    stats["timestamp"] = datetime.now().isoformat()
    stats["cache_ttl_seconds"] = _response_cache._default_ttl
    return json.dumps(stats, indent=2)


@mcp.tool()
def clear_cache() -> str:
    """
    Clear the response cache.

    Forces fresh data to be fetched on next request.
    Useful after model changes or configuration updates.
    """
    _response_cache.invalidate()
    return json.dumps({
        "timestamp": datetime.now().isoformat(),
        "status": "Cache cleared successfully",
        "message": "Next requests will fetch fresh data"
    }, indent=2)


@mcp.tool()
def scan_database_locations() -> str:
    """
    Scan for Msty database files in common locations.

    Useful for debugging when database is not found automatically.
    Shows all potential database files and their sizes.

    Returns:
        List of found database files with details
    """
    home = Path.home()
    result = {
        "timestamp": datetime.now().isoformat(),
        "scan_locations": [],
        "found_databases": [],
        "current_config": get_msty_paths()
    }

    # Locations to scan
    scan_paths = [
        home / "Library/Application Support/MstyStudio",
        home / "Library/Application Support/MstySidecar",
        home / "Library/Containers/ai.msty.MstyStudio",
        home / ".msty",
        home / ".config/msty",
    ]

    for scan_path in scan_paths:
        location_info = {"path": str(scan_path), "exists": scan_path.exists(), "databases": []}

        if scan_path.exists():
            try:
                # Find all database files
                for db_file in scan_path.glob("**/*.db"):
                    try:
                        stat = db_file.stat()
                        location_info["databases"].append({
                            "path": str(db_file),
                            "size_mb": round(stat.st_size / (1024 * 1024), 2),
                            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
                        })
                        result["found_databases"].append(str(db_file))
                    except:
                        pass

                for db_file in scan_path.glob("**/*.sqlite*"):
                    try:
                        stat = db_file.stat()
                        location_info["databases"].append({
                            "path": str(db_file),
                            "size_mb": round(stat.st_size / (1024 * 1024), 2),
                            "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
                        })
                        result["found_databases"].append(str(db_file))
                    except:
                        pass
            except PermissionError:
                location_info["error"] = "Permission denied"

        result["scan_locations"].append(location_info)

    result["total_databases_found"] = len(result["found_databases"])

    if result["found_databases"]:
        result["recommendation"] = f"Set MSTY_DATABASE_PATH environment variable to one of the found databases"
    else:
        result["recommendation"] = "No databases found. Msty 2.4.0+ may store data differently - check Msty settings or documentation"

    return json.dumps(result, indent=2)


# =============================================================================
# Phase 4: Intelligence Layer
# =============================================================================

@mcp.tool()
def get_model_performance_metrics(model_id: Optional[str] = None, days: int = 30) -> str:
    """
    Get performance metrics for local models over time.
    
    Args:
        model_id: Specific model to query (None = all models)
        days: Number of days to include in analysis (default: 30)
    
    Returns:
        Aggregated performance metrics with trends
    """
    result = {"timestamp": datetime.now().isoformat(), "period_days": days}
    
    try:
        init_metrics_db()
        metrics = get_model_metrics_summary(model_id=model_id, days=days)
        result["metrics"] = metrics
        
        if metrics.get("models"):
            insights = []
            for m in metrics["models"]:
                tps = m.get("avg_tokens_per_second", 0) or 0
                if tps > 50:
                    insights.append(f"âœ… {m['model_id']}: Excellent speed ({tps:.1f} tok/s)")
                elif tps > 20:
                    insights.append(f"ðŸ‘ {m['model_id']}: Good speed ({tps:.1f} tok/s)")
                elif tps > 0:
                    insights.append(f"âš ï¸ {m['model_id']}: Slow ({tps:.1f} tok/s)")
            result["insights"] = insights
    except Exception as e:
        result["error"] = str(e)
    
    return json.dumps(result, indent=2)


@mcp.tool()
def analyse_conversation_patterns(days: int = 30) -> str:
    """
    Analyse conversation patterns from Msty database.
    
    Privacy-respecting analysis that tracks session counts, message volumes,
    and model usage distribution without exposing conversation content.
    
    Args:
        days: Number of days to analyse (default: 30)
    
    Returns:
        Aggregated usage patterns
    """
    result = {"timestamp": datetime.now().isoformat(), "period_days": days, "patterns": {}}
    
    paths = get_msty_paths()
    db_path = paths.get("database")
    
    if not db_path:
        result["error"] = "Msty database not found"
        return json.dumps(result, indent=2)
    
    try:
        tables = get_table_names(db_path)
        patterns = {"session_analysis": {}, "model_usage": {}}
        
        for t in ["chat_sessions", "conversations"]:
            if t in tables:
                count_result = query_database(db_path, f"SELECT COUNT(*) as count FROM {t}")
                patterns["session_analysis"]["total_sessions"] = count_result[0]["count"] if count_result else 0
                
                recent = query_database(db_path, f"SELECT * FROM {t} ORDER BY rowid DESC LIMIT 100")
                if recent:
                    model_counts = {}
                    for s in recent:
                        model = s.get("model") or s.get("model_id") or s.get("llm_model") or "unknown"
                        model_counts[model] = model_counts.get(model, 0) + 1
                    patterns["model_usage"] = model_counts
                break
        
        result["patterns"] = patterns
    except Exception as e:
        result["error"] = str(e)
    
    return json.dumps(result, indent=2, default=str)


@mcp.tool()
def compare_model_responses(
    prompt: str,
    models: Optional[List[str]] = None,
    system_prompt: Optional[str] = None,
    evaluation_criteria: str = "balanced"
) -> str:
    """
    Send the same prompt to multiple models and compare responses.

    Args:
        prompt: The prompt to send to all models
        models: List of model IDs to compare (None = use all available chat models, max 5)
        system_prompt: Optional system prompt for context
        evaluation_criteria: What to optimise for ("quality", "speed", "balanced")

    Returns:
        Comparison of responses with timing and quality scores
    """
    result = {"timestamp": datetime.now().isoformat(), "prompt": prompt[:200] + "...", "responses": [], "comparison": {}}

    # Check if any service is available
    services = get_available_service_ports()
    any_available = any(s["available"] for s in services.values())

    if not any_available:
        result["error"] = "No Local AI service is running. Start Msty Studio and enable services."
        return json.dumps(result, indent=2)

    # Collect all chat models from all services if not specified
    if not models:
        all_models = []
        for service_name, service_info in services.items():
            if service_info["available"]:
                response = make_api_request("/v1/models", port=service_info["port"])
                if response.get("success"):
                    data = response.get("data", {})
                    if isinstance(data, dict) and "data" in data:
                        for m in data["data"]:
                            model_id = m.get("id", "")
                            # Skip embedding models
                            if "embed" in model_id.lower() or "bge" in model_id.lower() or "nomic" in model_id.lower():
                                continue
                            all_models.append({"id": model_id, "port": service_info["port"]})
        models = all_models[:5]  # Limit to 5 models
        if not models:
            result["error"] = "No chat models available"
            return json.dumps(result, indent=2)

    init_metrics_db()

    for model_entry in models:
        # Handle both dict format (from auto-discovery) and string format (user-provided)
        if isinstance(model_entry, dict):
            model_id = model_entry["id"]
            port = model_entry["port"]
        else:
            model_id = model_entry
            port = get_chat_port_for_model(model_id)

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        start_time = time.time()
        response = make_api_request("/v1/chat/completions", port=port, method="POST",
            data={"model": model_id, "messages": messages, "temperature": 0.7, "max_tokens": 1024, "stream": False}, timeout=120)
        elapsed = time.time() - start_time
        
        model_result = {"model_id": model_id, "success": response.get("success", False), "latency_seconds": round(elapsed, 2)}
        
        if response.get("success"):
            data = response.get("data", {})
            if "choices" in data and data["choices"]:
                content = data["choices"][0].get("message", {}).get("content", "") or data["choices"][0].get("message", {}).get("reasoning", "")
                model_result["response"] = content[:500] + "..." if len(content) > 500 else content
                model_result["response_length"] = len(content)
                
                if "usage" in data:
                    model_result["tokens_per_second"] = round(data["usage"].get("completion_tokens", 0) / max(elapsed, 0.1), 1)
                
                eval_result = evaluate_response_heuristic(prompt, content, "general")
                model_result["quality_score"] = round(eval_result["score"], 2)
                
                record_model_metric(model_id=model_id, completion_tokens=data.get("usage", {}).get("completion_tokens", 0),
                    latency_seconds=elapsed, success=True, use_case="comparison")
        
        result["responses"].append(model_result)
    
    successful = [r for r in result["responses"] if r["success"]]
    if successful:
        if evaluation_criteria == "speed":
            best = min(successful, key=lambda x: x["latency_seconds"])
        elif evaluation_criteria == "quality":
            best = max(successful, key=lambda x: x.get("quality_score", 0))
        else:
            best = max(successful, key=lambda x: x.get("quality_score", 0.5) * 0.6 + (1.0 / max(x["latency_seconds"], 0.1)) * 0.4)
        result["comparison"]["winner"] = best["model_id"]
    
    return json.dumps(result, indent=2, default=str)


@mcp.tool()
def optimise_knowledge_stacks() -> str:
    """
    Analyse and recommend optimisations for knowledge stacks.
    
    Returns:
        Recommendations for knowledge stack improvements
    """
    result = {"timestamp": datetime.now().isoformat(), "analysis": {}, "recommendations": []}
    
    paths = get_msty_paths()
    db_path = paths.get("database")
    
    if not db_path:
        result["error"] = "Msty database not found"
        return json.dumps(result, indent=2)
    
    try:
        tables = get_table_names(db_path)
        ks_table = None
        for t in ["knowledge_stacks", "knowledge_stack"]:
            if t in tables:
                ks_table = t
                break
        
        if not ks_table:
            result["note"] = "No knowledge stack table found"
            return json.dumps(result, indent=2)
        
        stacks = query_database(db_path, f"SELECT * FROM {ks_table}")
        result["analysis"]["total_stacks"] = len(stacks)
        
        if len(stacks) == 0:
            result["recommendations"].append("No knowledge stacks found. Consider creating domain-specific stacks.")
        elif len(stacks) > 10:
            result["recommendations"].append(f"Many stacks ({len(stacks)}). Consider consolidating.")
    except Exception as e:
        result["error"] = str(e)
    
    return json.dumps(result, indent=2, default=str)


@mcp.tool()
def suggest_persona_improvements(persona_name: Optional[str] = None) -> str:
    """
    Analyse personas and suggest improvements.
    
    Args:
        persona_name: Specific persona to analyse (None = all)
    
    Returns:
        Suggestions for persona optimisation
    """
    result = {"timestamp": datetime.now().isoformat(), "analysis": {}, "suggestions": []}
    
    paths = get_msty_paths()
    db_path = paths.get("database")
    
    if not db_path:
        result["error"] = "Msty database not found"
        return json.dumps(result, indent=2)
    
    try:
        tables = get_table_names(db_path)
        persona_table = "personas" if "personas" in tables else None
        
        if not persona_table:
            result["note"] = "No persona table found"
            return json.dumps(result, indent=2)
        
        if persona_name:
            personas = query_database(db_path, f"SELECT * FROM {persona_table} WHERE name LIKE ?", (f"%{persona_name}%",))
        else:
            personas = query_database(db_path, f"SELECT * FROM {persona_table}")
        
        result["analysis"]["total_personas"] = len(personas)
        
        for p in personas:
            name = p.get("name", "Unknown")
            prompt_len = len(p.get("system_prompt", "") or p.get("prompt", "") or "")
            temp = p.get("temperature")
            
            if prompt_len < 100:
                result["suggestions"].append(f"'{name}': System prompt too short.")
            elif prompt_len > 4000:
                result["suggestions"].append(f"'{name}': System prompt very long.")
            
            if temp is not None and temp > 0.9:
                result["suggestions"].append(f"'{name}': High temperature ({temp}) may cause inconsistency.")
        
        if len(personas) == 0:
            result["suggestions"].append("No personas found. Create task-specific personas.")
    except Exception as e:
        result["error"] = str(e)
    
    return json.dumps(result, indent=2, default=str)


# =============================================================================
# Phase 5: Tiered AI Workflow / Calibration
# =============================================================================

@mcp.tool()
def run_calibration_test(
    model_id: Optional[str] = None,
    category: str = "general",
    custom_prompt: Optional[str] = None,
    passing_threshold: float = 0.6
) -> str:
    """
    Run a calibration test on a local model.

    Args:
        model_id: Model to test (None = auto-select first available chat model)
        category: Test category ("general", "reasoning", "coding", "writing", "analysis", "creative")
        custom_prompt: Use a custom prompt instead of built-in tests
        passing_threshold: Minimum score to pass (0.0-1.0, default 0.6)

    Returns:
        Test results with quality scores and recommendations
    """
    result = {"timestamp": datetime.now().isoformat(), "category": category, "tests": [], "summary": {}}

    # Check if any service is available
    services = get_available_service_ports()
    any_available = any(s["available"] for s in services.values())

    if not any_available:
        result["error"] = "No Local AI service is running. Start Msty Studio and enable services."
        return json.dumps(result, indent=2)

    # Find model and port
    if model_id:
        port = get_chat_port_for_model(model_id)
    else:
        model_id, port = get_first_chat_model()
        if not model_id:
            result["error"] = "No chat models available (only embedding models found)"
            return json.dumps(result, indent=2)

    result["model_id"] = model_id
    result["port"] = port

    prompts_to_test = []
    if custom_prompt:
        prompts_to_test = [(category, custom_prompt)]
    elif category == "general":
        for cat, prompts in CALIBRATION_PROMPTS.items():
            if prompts:
                prompts_to_test.append((cat, prompts[0]))
    elif category in CALIBRATION_PROMPTS:
        for p in CALIBRATION_PROMPTS[category]:
            prompts_to_test.append((category, p))
    else:
        result["error"] = f"Unknown category: {category}"
        return json.dumps(result, indent=2)

    init_metrics_db()
    passed_count, total_score = 0, 0.0

    for test_cat, prompt in prompts_to_test:
        import hashlib
        test_id = hashlib.md5(f"{model_id}:{prompt}:{datetime.now().isoformat()}".encode()).hexdigest()[:12]

        start_time = time.time()
        response = make_api_request("/v1/chat/completions", port=port, method="POST",
            data={"model": model_id, "messages": [{"role": "user", "content": prompt}], "temperature": 0.7, "max_tokens": 1024, "stream": False}, timeout=120)
        elapsed = time.time() - start_time
        
        test_result = {"test_id": test_id, "category": test_cat, "prompt": prompt[:100] + "...", "latency_seconds": round(elapsed, 2)}
        
        if response.get("success"):
            data = response.get("data", {})
            if "choices" in data and data["choices"]:
                content = data["choices"][0].get("message", {}).get("content", "") or data["choices"][0].get("message", {}).get("reasoning", "")
                
                evaluation = evaluate_response_heuristic(prompt, content, test_cat)
                test_result["quality_score"] = round(evaluation["score"], 2)
                test_result["passed"] = evaluation["score"] >= passing_threshold
                test_result["notes"] = evaluation["notes"]
                
                total_score += evaluation["score"]
                if test_result["passed"]:
                    passed_count += 1
                
                save_calibration_result(test_id, model_id, test_cat, prompt, content, evaluation["score"], 
                    json.dumps(evaluation["notes"]), elapsed, test_result["passed"])
        else:
            test_result["error"] = response.get("error")
            test_result["passed"] = False
        
        result["tests"].append(test_result)
    
    total_tests = len(prompts_to_test)
    result["summary"] = {
        "total_tests": total_tests,
        "passed": passed_count,
        "pass_rate": round(passed_count / max(total_tests, 1) * 100, 1),
        "average_score": round(total_score / max(total_tests, 1), 2)
    }
    
    return json.dumps(result, indent=2, default=str)


@mcp.tool()
def evaluate_response_quality(prompt: str, response: str, category: str = "general") -> str:
    """
    Evaluate the quality of a model response.
    
    Args:
        prompt: The original prompt
        response: The model's response
        category: Response category for specific evaluation criteria
    
    Returns:
        Quality score (0.0-1.0) with detailed breakdown
    """
    evaluation = evaluate_response_heuristic(prompt, response, category)
    
    result = {
        "timestamp": datetime.now().isoformat(),
        "category": category,
        "quality_score": round(evaluation["score"], 3),
        "passed": evaluation["passed"],
        "criteria_scores": {k: round(v, 2) for k, v in evaluation.get("criteria_scores", {}).items()},
        "notes": evaluation["notes"],
        "rubric": QUALITY_RUBRIC
    }
    
    score = evaluation["score"]
    if score >= 0.8:
        result["interpretation"] = "Excellent"
    elif score >= 0.6:
        result["interpretation"] = "Good"
    elif score >= 0.4:
        result["interpretation"] = "Fair"
    else:
        result["interpretation"] = "Poor"
    
    return json.dumps(result, indent=2)


@mcp.tool()
def identify_handoff_triggers(
    analyse_recent: bool = True,
    add_pattern: Optional[str] = None,
    pattern_type: Optional[str] = None
) -> str:
    """
    Identify and manage patterns that should trigger escalation to Claude.
    
    Args:
        analyse_recent: Analyse recent calibration tests for triggers
        add_pattern: Manually add a trigger pattern description
        pattern_type: Type of pattern ("complexity", "domain", "quality", "safety", "creativity")
    
    Returns:
        List of identified handoff triggers with confidence scores
    """
    result = {"timestamp": datetime.now().isoformat(), "triggers": [], "analysis": {}}
    
    init_metrics_db()
    
    if add_pattern and pattern_type:
        record_handoff_trigger(pattern_type, add_pattern, 0.7)
        result["added_pattern"] = {"type": pattern_type, "description": add_pattern}
    
    if analyse_recent:
        calibration_results = get_calibration_results(limit=100)
        failed = [r for r in calibration_results if not r.get("passed")]
        
        category_failures = {}
        for test in failed:
            cat = test.get("prompt_category", "unknown")
            category_failures[cat] = category_failures.get(cat, 0) + 1
        
        result["analysis"]["failed_tests_count"] = len(failed)
        result["analysis"]["failure_by_category"] = category_failures
        
        for cat, count in category_failures.items():
            if count >= 3:
                record_handoff_trigger("category_failure", f"Local model fails {cat} tasks", min(count / 10, 1.0))
    
    result["triggers"] = get_handoff_triggers(active_only=True)
    result["trigger_count"] = len(result["triggers"])
    
    return json.dumps(result, indent=2, default=str)


@mcp.tool()
def get_calibration_history(
    model_id: Optional[str] = None,
    category: Optional[str] = None,
    limit: int = 50
) -> str:
    """
    Get historical calibration test results.
    
    Args:
        model_id: Filter by specific model (None = all)
        category: Filter by test category (None = all)
        limit: Maximum results to return (default: 50)
    
    Returns:
        Historical test results with trends and statistics
    """
    result = {"timestamp": datetime.now().isoformat(), "filters": {"model_id": model_id, "category": category}, "history": [], "statistics": {}}
    
    init_metrics_db()
    
    all_results = get_calibration_results(model_id=model_id, limit=limit)
    
    if category:
        all_results = [r for r in all_results if r.get("prompt_category") == category]
    
    result["history"] = all_results
    result["total_tests"] = len(all_results)
    
    if all_results:
        scores = [r.get("quality_score", 0) for r in all_results if r.get("quality_score")]
        passed = sum(1 for r in all_results if r.get("passed"))
        
        result["statistics"] = {
            "average_score": round(sum(scores) / len(scores), 2) if scores else 0,
            "pass_count": passed,
            "pass_rate": round(passed / len(all_results) * 100, 1)
        }
    else:
        result["note"] = "No calibration tests found. Run run_calibration_test to generate data."
    
    return json.dumps(result, indent=2, default=str)


# =============================================================================
# Main Entry Point
# =============================================================================

def main():
    """Run the Msty Admin MCP server"""
    logger.info(f"Starting Msty Admin MCP Server v{SERVER_VERSION}")
    logger.info("Phase 1: Foundational Tools (Read-Only)")
    logger.info("Phase 2: Configuration Management")
    logger.info("Phase 3: Automation Bridge")
    logger.info("Phase 4: Intelligence Layer")
    logger.info("Phase 5: Tiered AI Workflow")
    logger.info("Total tools: 24")
    mcp.run()


if __name__ == "__main__":
    main()
